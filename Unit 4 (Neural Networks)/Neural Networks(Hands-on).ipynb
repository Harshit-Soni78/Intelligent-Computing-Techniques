{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unit 4: Neural Networks**\n",
    "\n",
    "## **1. Introduction to Neural Networks**\n",
    "\n",
    "Artificial Neural Networks (ANNs) are computational models inspired by the human brain. They consist of layers of interconnected neurons that process data and learn patterns.\n",
    "\n",
    "### **Key Components of a Neural Network:**\n",
    "\n",
    "- **Neuron (Perceptron):** Basic unit of computation.\n",
    "- **Weights & Biases:** Parameters that adjust during learning.\n",
    "- **Activation Function:** Determines neuron output.\n",
    "- **Layers:**\n",
    "  - **Input Layer**: Takes raw data as input.\n",
    "  - **Hidden Layers**: Process and learn patterns.\n",
    "  - **Output Layer**: Provides the final result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Structure and Function of a Single Neuron**\n",
    "\n",
    "A single-layer perceptron computes a weighted sum of inputs and applies an activation function.\n",
    "\n",
    "### **Mathematical Model:**\n",
    "\n",
    "$$ y = f\\left(\\sum(w_i \\cdot x_i) + b\\right) $$\n",
    "where:\n",
    "\n",
    "- $$ x_i = Inputs  $$\n",
    "- $$ w_i = Weights $$  \n",
    "- $$ b = Bias  $$\n",
    "- $$ f = Activation Function  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Hands-on: Implementing a Perceptron from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install numpy\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, lr=0.1, epochs=10):\n",
    "        self.weights = np.random.randn(input_size + 1)  # Including bias\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.c_[X, np.ones(X.shape[0])]  # Adding bias term\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                y_pred = self.activation(np.dot(self.weights, X[i]))\n",
    "                error = y[i] - y_pred\n",
    "                self.weights += self.lr * error * X[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[X, np.ones(X.shape[0])]\n",
    "        return np.array([self.activation(np.dot(self.weights, x)) for x in X])\n",
    "\n",
    "# Example: OR Logic Gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])\n",
    "\n",
    "model = Perceptron(input_size=2)\n",
    "model.train(X, y)\n",
    "print(\"Predictions:\", model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** A perceptron is trained on OR gate logic using simple weight updates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Multi-Layer Perceptron (MLP)**\n",
    "\n",
    "MLPs consist of multiple layers with nonlinear activation functions to learn complex patterns.\n",
    "\n",
    "### **Common Activation Functions:**\n",
    "\n",
    "- **Sigmoid:** $$ f(x) = \\frac{1}{1+e^{-x}} $$  \n",
    "- **ReLU:** $$ f(x) = \\max(0, x) $$  \n",
    "- **Softmax:** Typically used in multi-class classification, expressed as $$ f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $$\n",
    "\n",
    "### **Implementing a Multi-Layer Perceptron using TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tensorflow\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data: XOR Gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Building the MLP Model\n",
    "model = Sequential([\n",
    "    Dense(4, activation='relu', input_shape=(2,)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "print(\"Predictions:\", model.predict(X).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** An MLP learns the XOR function using a hidden layer with ReLU activation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Differences Between ANN and Human Brain**\n",
    "\n",
    "| Feature          | Artificial Neural Network | Human Brain               |\n",
    "| ---------------- | ------------------------- | ------------------------- |\n",
    "| Learning         | Supervised, Reinforcement | Unsupervised, Adaptive    |\n",
    "| Processing Speed | Fast for computation      | Slower but more efficient |\n",
    "| Parallelism      | Limited                   | Highly parallel           |\n",
    "| Generalization   | Requires large data       | Learns from few examples  |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Applications of Neural Networks**\n",
    "\n",
    "- Image Recognition (e.g., Face Detection)\n",
    "- Speech Recognition (e.g., Google Assistant, Siri)\n",
    "- Fraud Detection (e.g., Credit Card Fraud)\n",
    "- Autonomous Vehicles (e.g., Self-driving cars)\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Discussion Questions**\n",
    "\n",
    "1. Why do neural networks require activation functions?\n",
    "2. What challenges exist in training deep neural networks?\n",
    "3. How does backpropagation optimize neural network weights?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
